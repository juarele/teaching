{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps in PySpark \n",
    "\n",
    "In this notebook we will learn the fundamentals of functional programming, as well as the basic abstraction of a distributed object in Spark, the RDD. The notebook has been divided into two parts:\n",
    "\n",
    "Part 1: map/reduce basics\n",
    "\n",
    "Part 2: Work with RDD and Pair RDD abstractions \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: map/reduce basics\n",
    "\n",
    "![Hadoop Logo](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Hadoop_logo.svg/220px-Hadoop_logo.svg.png)\n",
    "# **Apache Hadoop (MapReduce)**\n",
    "\n",
    "It is an open source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures (of individual machines, or racks of machines) are common and thus should be automatically handled in software by the framework.\n",
    "\n",
    "The core of Apache Hadoop consists of a storage part (Hadoop Distributed File System (HDFS)) and a processing part (MapReduce). Hadoop splits files into large blocks and distributes them amongst the nodes in the cluster. To process the data, Hadoop MapReduce transfers packaged code for nodes to process in parallel, based on the data each node needs to process. This approach takes advantage of data locality — nodes manipulating the data that they have on hand — to allow the data to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are connected via high-speed networking.\n",
    "\n",
    "![caption](http://d152j5tfobgaot.cloudfront.net/wp-content/uploads/2012/07/mapreduce.png)\n",
    "\n",
    "Since data and computation are distributed, we should avoid the use of variables, i.e. mutable data. Thus, in contrast to impertaive programming, we shall use the functional approach (lambda calculus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of the following excercises is to understand basic lambda calculus with python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Functional programming in Python\n",
    "\n",
    "So, what is Functional Programming? From Wikipedia: \n",
    "\n",
    "« …a  programing paradigm that treats computation as the evaluation of mathematical functions and **avoids changing-state and mutable  data**.»\n",
    "\n",
    "It´s based upon Lambda calculus, wich consist of:\n",
    " * Function definition (declaration of expressions)\n",
    " * Function application (evaluation of those expressions)\n",
    " * Recursion (iteration)\n",
    "\n",
    "We have already used this in python!!! :)\n",
    "\n",
    "Recall the typical \"lambda x: x+1\" we have been using as the first argument of map, reduce and filter methods:\n",
    " * **map** maps each value in the input collection to a different value. It´s just the classical mathematical funciton we are used to!\n",
    " * **reduce** takes two values from the input collection and returns a new value (of the same type) by appliying a commutative operation to them. \n",
    " * **filter** filters the elements in the input collection according to a certain (boolean) criteria.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapping**\n",
    "\n",
    "![map](https://cosminpupaza.files.wordpress.com/2015/10/map.png?w=505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = range(1,10)\n",
    "map(lambda x: x+1,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# square elements in a\n",
    "map(lambda y: y**2,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering**\n",
    "![filter](https://cosminpupaza.files.wordpress.com/2015/11/filter.png?w=405)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter elements equal 2\n",
    "filter(lambda x: x==2,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out elements multiple of 3\n",
    "filter(lambda x: x%3==0,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing** Recall it must be commutative! Think about the importance of this when parallelizing computations\n",
    "\n",
    "![](https://cosminpupaza.files.wordpress.com/2015/11/reduce.png?w=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add all elements in the collection\n",
    "reduce(lambda x,y: x+y,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fin the maximum in the collection\n",
    "reduce(lambda x,y: x if x>y else y,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-43\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# subtract all elements in the collection: this is not commutative\n",
    "print reduce(lambda x,y: x-y,a)\n",
    "print reduce(lambda x,y: y-x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4\n",
      "-6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# subtract all elements in the collection. We can give a default value to start with.\n",
    "print reduce(lambda x,y: x-y,[1,2,3])\n",
    "print reduce(lambda x,y: x-y,[1,2,3],0)\n",
    "print reduce(lambda x,y: x-y,[1,2,3],6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1b) Exercise: Calculate the mean of a collection of real numbers using map/reduce\n",
    "Recall:\n",
    "\n",
    "$$\\bar x = \\frac{\\sum_{i=1}^{N} x_i}{N} $$\n",
    "\n",
    "It´s starightforward to do this with python built-in methods sum() and len(). However, how would you do that with map/reduce? We have already shown how to usm the elements of an array. Thus, you have to calculate the length of the array. For this:\n",
    " * Create another array of the same size, consisting of 1s.\n",
    " * Sum the elements of that array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mean calculation: we need the sum and the number of elements\n",
    "a = range(1,10)\n",
    "\n",
    "ocurrancies = map(lambda x: 1,a)\n",
    "count = reduce(lambda x,y: x+y,ocurrancies)\n",
    "mean = reduce(lambda x,y: x+y,a)/count\n",
    "\n",
    "assert len(ocurrancies) == len(a)\n",
    "assert count == len(a)\n",
    "assert mean == sum(a)/len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1c) Exercise: Calculate the standard deviation of a collection of real numbers\n",
    "Recall:\n",
    "\n",
    "$$\\sigma_x^2 = \\frac{\\sum_{i=1}^{N} (x_i-\\bar x)^2}{N-1} =\n",
    "\\frac{\\sum_{i=1}^{N}\\left(x_i^2+\\bar x ^2-2x_i\\bar x\\right)}{N-1} = \n",
    "\\frac{\\sum_{i=1}^{N} x_i^2-N\\bar x^2}{N-1}$$\n",
    "\n",
    "For this, use the *mean* and *count* variables from the previous excercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# std calculation: apart from the mean and count, the sum of the squares is required.\n",
    "squares = map(lambda x: x**2,a)\n",
    "sum_squares = reduce(lambda x,y:x+y,squares)\n",
    "std = pow((sum_squares-count*(mean**2))/float(count-1),0.5)\n",
    "\n",
    "assert std == 7.5**0.5\n",
    "\n",
    "# another way:\n",
    "dev = map(lambda x: float((x-mean)**2),a)\n",
    "std = reduce(lambda i,j: i+j,dev)/(count-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1c.bis) Exercise: all at once! \n",
    "For the std calculation, we have obtained separatedly the sum of elements, the lenght and the sum of the elements squared. That is, we have swept the array three times! Can you do it in a two step process using map/reduce? Do you think it might matter at some point?\n",
    " * Hint: recall that reduce takes two arguments of the same type, and returns another value of that type. So, instead of using numbers as the elements of our array, use tuples!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 45, 285)\n"
     ]
    }
   ],
   "source": [
    "a = range(1,10)\n",
    "\n",
    "map_tuples = map(lambda j: (1,j,j**2),a)\n",
    "reduce_tuples = reduce(lambda i,j:(i[0]+j[0],i[1]+j[1],i[2]+j[2]),map_tuples)\n",
    "\n",
    "print reduce_tuples\n",
    "count = reduce_tuples[0]\n",
    "mean = reduce_tuples[1]/float(count)\n",
    "sum_sq = reduce_tuples[2]\n",
    "\n",
    "std = pow((sum_squares-count*(mean**2))/float(count-1),0.5)\n",
    "\n",
    "assert std == 7.5**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1d) Twe 'word-count' problem: creating histograms\n",
    "Given a set of keys (e.g. words) in an input collection, calculate the frequency of each key (word). \n",
    "\n",
    "In order to understand better how map/reduce works, we will implement this simple calculation in several forms.\n",
    "\n",
    "For simplicity, we are going to create a list of numbers between 1 and 9, that can be repated a (random) number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 7, 1, 6, 2, 1, 5, 5, 5, 4, 2, 8, 5, 9, 9, 2, 8, 1, 5, 7, 6, 9, 6, 2, 2, 2, 7, 5, 6, 5, 8, 4, 5, 8, 2, 6, 5, 6, 5, 5, 7, 6, 5, 4, 6, 9, 5, 6, 8, 8, 8, 8, 7, 8, 5, 5, 5, 6, 7, 7, 6, 2, 8, 9, 4, 3, 7, 7, 5, 5, 5, 5, 5, 5, 9, 5, 5, 2, 8, 5, 2, 2, 8, 8, 6, 2, 8, 2, 4, 8, 2, 6, 9, 2, 7, 8, 6, 7, 6, 5, 8, 4, 5, 7, 9, 4, 8, 5, 5, 7, 8, 6, 8, 9, 1, 9, 4, 6, 9, 5, 8, 6, 2, 2, 2, 5, 9, 6, 1, 2, 5, 9, 9, 9, 2, 9, 6, 5, 8, 2, 8, 6, 9, 8, 8, 1, 9, 6, 4, 2, 9, 2, 5, 2, 2, 2, 8, 8, 5, 6, 7, 5, 5, 3, 7, 8, 6, 3, 2, 8, 8, 7, 8, 5, 2, 4, 8, 7, 5, 5, 9, 5, 2, 5, 1, 8, 4, 5, 8, 8, 4, 8, 8, 7, 5, 6, 5, 7, 2, 7, 5, 2, 7, 5, 8, 9, 3, 9, 9, 6, 5, 6, 9, 8, 2, 5, 7, 6, 7, 6, 2, 7, 9, 2, 6, 8, 4, 1, 8, 8, 2, 5, 8, 8, 8, 6, 8, 5, 2, 8, 7, 2, 5, 6, 2, 8, 9, 8, 2, 9, 9, 6, 6, 5, 6, 2, 9, 1, 8, 2, 8, 8, 6, 9, 8, 6, 4, 8, 6, 2, 8, 9, 5, 6, 7, 2, 7, 2, 2, 9, 5, 9, 6, 5, 7, 6, 5, 5, 9, 6, 2, 5, 5, 5, 3, 7, 2, 5, 4, 6, 7, 5, 6, 1, 7, 7, 9, 2, 2, 2, 2, 2, 5, 4, 2, 6, 2, 9, 8, 6, 6, 2, 2, 6, 9, 9, 1, 5, 5, 7, 9, 7, 1, 5, 8, 5, 8, 6, 8, 2, 9, 8, 6, 9, 5, 1, 2, 9, 9, 8, 9, 8, 6, 6, 2, 2, 2, 9, 8, 8, 2, 5, 7, 2, 2, 4, 5, 2, 6, 6, 7, 7, 6, 6, 6, 9, 5, 8, 5, 5, 8, 5, 1, 4, 8, 6, 8, 9, 6, 2, 1, 5, 8, 8, 8, 5, 9, 2, 2, 2, 8, 6, 3, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "# Create array of numbers with repeated elements\n",
    "import random\n",
    "a = range(1,10)\n",
    "a = [[x]*random.randint(1,100) for x in a]\n",
    "a = [x for y in a for x in y]\n",
    "random.shuffle(a)\n",
    "print a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d.1) Simple approach\n",
    "\n",
    " * Start with an empty dict\n",
    " * If a new key is not present in the dict, create it.\n",
    " * Otherwise, increase the frequency of the key by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 15, 2: 73, 3: 6, 4: 18, 5: 78, 6: 60, 7: 37, 8: 72, 9: 48}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def histSimple(lista):\n",
    "    # create empty dictionary\n",
    "    hist = {}\n",
    "    \n",
    "    for key in lista:\n",
    "        if key in hist: hist[key] +=1 \n",
    "        else: hist[key] = 1\n",
    "    return hist  \n",
    "\n",
    "histSimple(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d.2) Map/reduce\n",
    "\n",
    " * Recall that *reduce* applies an operation to 2 elements of the same type, and returns another element of that type. Thus, first thing to do is to map our collection to the type of the output. We cannot use dicts, as dict(list) removes duplictaed keys. We will use list of tuples instead.\n",
    " * Then, we have to define a mehtod in the reducer that combines keys. There are two steps:\n",
    "   * Obtain the keys in the left list\n",
    "   * Then, check that the key in the second list already exists in the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 73), (7, 37), (1, 15), (6, 60), (5, 78), (4, 18), (8, 72), (9, 48), (3, 6)]\n"
     ]
    }
   ],
   "source": [
    "# Same histogram with map/reduce\n",
    "def histMR(l,verbose=False):\n",
    "    # emit an array of list of tuples\n",
    "    m = map(lambda i: [(i,1)],a) \n",
    "    return reduce(lambda i,j: combineKeys(i,j,verbose), m)\n",
    "\n",
    "def combineKeys(l1,l2,verbose=False):\n",
    "    '''\n",
    "    method to combine keys\n",
    "    \n",
    "    inputs: \n",
    "        * l1: list of tuples [(k1,v1),(k2,v2),...]        \n",
    "        * l2: list of single tuple [(kk,vv)]\n",
    "    \n",
    "    output: list of tuples [(k1,f1),(k2,f2),...].\n",
    "    '''\n",
    "    \n",
    "    # It is useful to print the process\n",
    "    if verbose:\n",
    "        print \"reducing\"\n",
    "        print l1\n",
    "        print l2\n",
    "        \n",
    "    # keys in left list\n",
    "    keys1 = map(lambda (key,value): key,l1)\n",
    "    # key in right list\n",
    "    k2 = l2[0][0]\n",
    "    if k2 in keys1:\n",
    "        # get the index of the key=k2 in keys1\n",
    "        index = keys1.index(k2)\n",
    "        # increase the value of that key accordingly\n",
    "        l1[index] = (k2,l1[index][1]+l2[0][1])\n",
    "    else:\n",
    "        # append the missing (key, value) pair to the left list\n",
    "        l1 = l1+l2\n",
    "    return l1\n",
    "\n",
    "# if you want to see how works the reducer, call histMR with verbose=True, i.e. histMR(a,True)\n",
    "print histMR(a)\n",
    "\n",
    "assert histSimple(a) == dict(histMR(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference with the previous method, based on dictionaries: now, keys are not sorted!!\n",
    "\n",
    "But, where did we sorted the keys in *histSimple*??? Well, we didn´t, but python *dictionary* does that internally for us to speed up things. See the difference in time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 73), (7, 37), (1, 15), (6, 60), (5, 78), (4, 18), (8, 72), (9, 48), (3, 6)]\n",
      "{1: 15, 2: 73, 3: 6, 4: 18, 5: 78, 6: 60, 7: 37, 8: 72, 9: 48}\n",
      "10000 loops, best of 3: 50.7 µs per loop\n",
      "1000 loops, best of 3: 959 µs per loop\n"
     ]
    }
   ],
   "source": [
    "print histMR(a)\n",
    "print dict(histMR(a))\n",
    "%timeit histSimple(a)\n",
    "%timeit histMR(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1d.3) Map/reduce with pre-sorting**  As shown, the sorting of keys used by a dictionary actually speed up the process. \n",
    "\n",
    "However, our *combineKeys* method is creating an array of keys and checking whether a new key is already present in every step. This can be avoid by sorting the initial list first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 15), (2, 73), (3, 6), (4, 18), (5, 78), (6, 60), (7, 37), (8, 72), (9, 48)]\n"
     ]
    }
   ],
   "source": [
    "# Same histogram with map/reduce and pre-sorting\n",
    "def histMRwithSort(l):\n",
    "    # sort the original list in ascending order\n",
    "    m = map(lambda i: [(i,1)],sorted(a,reverse=False)) \n",
    "    return reduce(combineSortedKeys, m)\n",
    "\n",
    "def combineSortedKeys(l1,l2):\n",
    "    '''\n",
    "    method to combine keys\n",
    "    \n",
    "    inputs: \n",
    "        * l1: list of tuples [(k1,v1),(k2,v2),...]        \n",
    "        * l2: list of single tuple [(kk,vv)]\n",
    "    \n",
    "    output: list of tuples [(k1,f1),(k2,f2),...].\n",
    "    '''\n",
    "    \n",
    "    # last key in left list\n",
    "    k1 = l1[-1][0]\n",
    "    # key in right list (there is only one!)\n",
    "    k2 = l2[0][0]\n",
    "    if k2 == k1:\n",
    "        # update the value in the left list corresponding to the last key\n",
    "        l1[-1] = (k2,l1[-1][1]+l2[0][1])\n",
    "    else:\n",
    "        # append the missing (key, value) pair to the left list\n",
    "        l1 = l1+l2\n",
    "    return l1\n",
    "\n",
    "print histMRwithSort(a)\n",
    "assert histSimple(a) == dict(histMRwithSort(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, computing times get closer. Still, our map/reduce methods are slower, since we cannot use dictionaries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the histogram with the simple approach\n",
      "10000 loops, best of 3: 52.1 µs per loop\n",
      "Calculating the histogram with MR\n",
      "1000 loops, best of 3: 962 µs per loop\n",
      "Calculating the histogram with MR after sorting\n",
      "1000 loops, best of 3: 298 µs per loop\n"
     ]
    }
   ],
   "source": [
    "print 'Calculating the histogram with the simple approach'\n",
    "#print hist(a)\n",
    "%timeit histSimple(a)\n",
    "\n",
    "print 'Calculating the histogram with MR'\n",
    "#print histMR(a)\n",
    "%timeit histMR(a)\n",
    "\n",
    "print 'Calculating the histogram with MR after sorting'\n",
    "#print histMR2(a)\n",
    "%timeit histMRwithSort(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2: Spark. Work with RDD and Pair RDD abstractions \n",
    "\n",
    "![drawing](https://prateekvjoshi.files.wordpress.com/2015/10/1-main4.png)\n",
    "\n",
    "# ** Apache Spark**\n",
    "\n",
    "Apache Spark is an open source cluster computing framework originally developed in the AMPLab at University of California, Berkeley but was later donated to the Apache Software Foundation where it remains today. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's multi-stage in-memory primitives provides performance up to 100 times faster for certain applications.\n",
    "\n",
    "![](http://image.slidesharecdn.com/sparkandshark-120620130508-phpapp01/95/spark-and-shark-8-728.jpg?cb=1340197567)\n",
    "\n",
    "By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms.\n",
    "![](http://spark.apache.org/images/logistic-regression.png)\n",
    "\n",
    "Spark comes with a number of components that provide flexibility and generality.\n",
    "\n",
    "<img src=\"http://spark.apache.org/images/spark-stack.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## In this part, we keep on working on the word-count example, this time with spark. The basic abstraction of Spark is the Resilient Distributed Dataset (RDD):\n",
    "\n",
    "#### «RDDs are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators.»\n",
    "\n",
    " * Read only, partitioned collection of records (immutable).\n",
    " * Stores the transformations used to build a dataset (its linage), instead of the data itself. This property ensures fault-tolerance.\n",
    " * User can control partitioning and persistence (caching).\n",
    " * RDDs are statically typed.\n",
    " * … and yes, everything is written in scala ;p. So you better learn a little bit of it!\n",
    " \n",
    "<img src=\"http://eng.trueaccord.com/wp-content/uploads/2014/10/scala-logo.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "#### We will be trying to understand this abstraction with simple examples, using the [Python API](http://spark.apache.org/docs/latest/api/python/index.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2a) Create a base RDD: parallelize, actions and transformations **\n",
    "We'll start by generating a base RDD by using a Python list and the `sc.parallelize` method.  Then we'll print out the type of the base RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList)\n",
    "# Print out the type of wordsRDD\n",
    "print type(wordsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nothing has actually happened!**\n",
    "\n",
    "`parallellize` tells spark to distribute the data, but this is not actually done until we perform some action.\n",
    "\n",
    "Possible actions include couting, collecting, reducing, taking, etc. Take a look at http://spark.apache.org/docs/latest/programming-guide.html#actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'elephant', 'rat', 'rat', 'cat']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at what is going on in the shell. Have a look also at localhost:4040\n",
    "wordsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'elephant']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from actions, we can apply transformations to an RDD. Spark won´t do anything, until an action is performed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[3] at RDD at PythonRDD.scala:43\n"
     ]
    }
   ],
   "source": [
    "# Apply a lambda function to our RDD to get the plural of each word\n",
    "pluralRDD = wordsRDD.map(lambda s: s+'s')\n",
    "print pluralRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'elephants', 'rats', 'rats', 'cats']\n"
     ]
    }
   ],
   "source": [
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we can obtain the length of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "pluralLengths = (pluralRDD\n",
    "                 .map(lambda w: len(w))\n",
    "                 .collect())\n",
    "print pluralLengths\n",
    "assert pluralLengths == [4,9,4,4,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2b) Persisting and the RDD lineage**\n",
    "\n",
    "So far, we have seen that Spark RDDs are *lazy evaluated*, i.e. nothing is actually done until an action is performed. In the RDD, the set of transformations to be applied are remembered: this is known as its *lineage*. It has the important consequence of making Spark RDDs *fault tolerant* automatically.\n",
    "\n",
    "![](http://images.slideplayer.com/14/4499833/slides/slide_10.jpg) \n",
    "\n",
    "It might be interesting to store some intermediate results, though: perhaps because we want to apply several different transformations starting from that point, or because we are going to apply an iterative computation (as is customary in machine learning algorithms). For this, Spark has [several ways of persisting](http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# note that the RDD is not cached until the first action is performed!! \n",
    "# Take a look at http://localhost:4040/storage/\n",
    "wordsCached = wordsRDD.cache()\n",
    "print wordsCached.is_cached\n",
    "print wordsCached.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'elephants', 'rats', 'rats', 'cats']\n",
      "['cat is an animal', 'elephant is an animal', 'rat is an animal', 'rat is an animal', 'cat is an animal']\n"
     ]
    }
   ],
   "source": [
    "print wordsCached.map(lambda s: s+'s').collect()\n",
    "print wordsCached.map(lambda s: s+' is an animal').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Deserialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# default persisting\n",
    "wordsCached.unpersist()\n",
    "wordsCached.persist(StorageLevel.MEMORY_ONLY)\n",
    "wordsCached.collect()\n",
    "print wordsCached.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# play with other levels of storage, and see how it changes in http://localhost:4040/storage/\n",
    "wordsCached.unpersist()\n",
    "wordsCached.persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "wordsCached.collect()\n",
    "print wordsCached.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsCached.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2c) Partitioning **\n",
    "\n",
    "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster.\n",
    "\n",
    "To get the number of partitions of an RDD, just use `getNumPartitions()` on your RDD. You can change the partitions during RDD creation (with `parallelize(collection,numPartitions)` or `fromTextFile(file,numPartitions)`), or afterwards with methos like `repartition(), coalesce()`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rat', 'cat', 'elephant', 'cat', 'rat']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the number of partitions\n",
    "wordRepartition = wordsRDD.repartition(5)\n",
    "wordRepartition.getNumPartitions()\n",
    "\n",
    "# in order to see the efects in the browser, we cache an apply an action\n",
    "wordRepartition.cache().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the partitions using [glom()](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=glom#pyspark.RDD.glom): it returns an RDD created by coalescing all elements within each partition into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rat'], ['cat', 'elephant', 'cat'], [], [], ['rat']]\n",
      "[['cat'], ['elephant'], ['rat'], ['rat', 'cat']]\n",
      "[['cat', 'elephant'], ['rat', 'rat', 'cat']]\n"
     ]
    }
   ],
   "source": [
    "print wordRepartition.glom().collect()\n",
    "print wordsRDD.glom().collect()\n",
    "print wordsRDD.coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitions are one of the most powerfull concepts in Spark: you can decide how to distribute your data so it can fit in memory, and more importantly, you can perform computations on each partition *before* speaking to other partitions. This can have an enorumous impact on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 loops, best of 5: 1.54 s per loop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[15] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize(range(1,10**7),4).cache()\n",
    "%timeit -n 3 -r 5 a.map(lambda x: x**2).sum()\n",
    "a.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 loops, best of 5: 951 ms per loop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[32] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize(xrange(1,10**7),4).cache()\n",
    "%timeit -n 3 -r 5  a.mapPartitions(lambda iter: [sum(i**2 for i in iter)]).sum()\n",
    "a.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 loops, best of 5: 952 ms per loop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[49] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize(xrange(1,10**7),8).cache()\n",
    "%timeit -n 3 -r 5  a.mapPartitions(lambda iter: [sum(i**2 for i in iter)]).sum()\n",
    "a.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2c) Pair RDDs: *grouping* strategies in Spark**\n",
    "\n",
    "The next step in writing our word counting program is to create a new type of RDD, called a pair RDD. A pair RDD is an RDD where each element is a pair tuple (k, v) where k is the key and v is the value. In this example, we will create a pair consisting of (`word`, 1) for each word element in the RDD, as we did in the map/reduce version of the histogram in Python, section (1d.2).\n",
    "\n",
    "We can create the pair RDD using the map() transformation with a lambda() function to create a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordPairs = wordsRDD.map(lambda w: (w,1))\n",
    "print wordPairs.collect()\n",
    "assert wordPairs.collect() == [('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2c.1) `groupByKey()` approach **\n",
    "An approach you might first consider (we'll see shortly that there are better ways) is based on using the [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) transformation. As the name implies, the `groupByKey()` transformation groups all the elements of the RDD with the same key into a single list in one of the partitions. There are two problems with using `groupByKey()`:\n",
    "  + The operation requires a lot of data movement to move all the values into the appropriate partitions.\n",
    "  + The lists can be very large. Consider a word count of English Wikipedia: the lists for common words (e.g., the, a, etc.) would be huge and could exhaust the available memory in a worker.\n",
    " \n",
    "![](http://blog.cheyo.net/static/file/spark_groupByKey.jpg)\n",
    "\n",
    "Use `groupByKey()` to generate a pair RDD of type `('word', iterator)`. Next, sum the iterator using a `map()` transformation.  The result should be a pair RDD consisting of (word, count) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rat: [1, 1]\n",
      "elephant: [1]\n",
      "cat: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Note that groupByKey requires no parameters\n",
    "wordsGrouped = wordPairs.groupByKey()\n",
    "# Print the key and the values corresponding to that word\n",
    "for key, value in wordsGrouped.collect():\n",
    "    print '{0}: {1}'.format(key,list(value))\n",
    "    \n",
    "assert ( sorted(wordsGrouped.mapValues(lambda x: list(x)).collect()) ==\n",
    "        [('cat', [1, 1]), ('elephant', [1]), ('rat', [1, 1])] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "wordCountsGrouped = wordsGrouped.map(lambda (k,iterator): (k,sum(iterator)))\n",
    "# also we can mapValues directly:\n",
    "wordCountsGrouped = wordsGrouped.mapValues(lambda iterator: sum(iterator))\n",
    "print wordCountsGrouped.collect()\n",
    "\n",
    "assert sorted(wordCountsGrouped.collect()) == [('cat', 2), ('elephant', 1), ('rat', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2c.2)  `reduceByKey` approach **\n",
    "A better approach is to start from the pair RDD and then use the [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) transformation to create a new pair RDD. \n",
    "\n",
    "The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets.\n",
    "\n",
    "![](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n",
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "wordCounts = wordPairs.reduceByKey(lambda a,b:a+b)\n",
    "print wordCounts.collect()\n",
    "\n",
    "# with add operator\n",
    "from operator import add\n",
    "wordCountsMod = wordPairs.reduceByKey(add)\n",
    "print wordCountsMod.collect()\n",
    "\n",
    "assert sorted(wordCounts.collect()) == [('cat', 2), ('elephant', 1), ('rat', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# All together: create tuples of (word,1), and then apply the reduceByKey method\n",
    "# to obtain the frequency of each word:\n",
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda x: (x,1))\n",
    "                       .reduceByKey(add)\n",
    "                       .collect())\n",
    "print wordCountsCollected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2c.3)  `combineByKey` approach: the mother of dragons **\n",
    "\n",
    "The [combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None)](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=combinebykey#pyspark.RDD.combineByKey) method is a generic (and powerful!)function to combine the elements for each key using a custom set of aggregation functions.\n",
    "\n",
    "It turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a “combined type” C. Note that V and C can be different – for example, one might group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).\n",
    "\n",
    "Users provide three functions:\n",
    "\n",
    "#### * createCombiner, which turns a V into a C (e.g., creates a one-element list)\n",
    "#### * mergeValue, to merge a V into a C (e.g., adds it to the end of a list)\n",
    "#### * mergeCombiners, to combine two C’s into a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 2, 'elephant': 1, 'rat': 2}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's count the number of words:\n",
    "wordPairs = wordsRDD.map(lambda i:(i,1))\n",
    "wordPairs.combineByKey(\n",
    "    # combiner: in this case we just return the integer\n",
    "    lambda integ: integ,                   \n",
    "    # merge value: how to add a new element (integer) to \n",
    "    # the combined value (integer again, in this simple case)\n",
    "    lambda count,integ: count+integ,       \n",
    "    # merge combiners: adding to combiners\n",
    "    lambda count1,count2: count1+count2\n",
    ").collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': (2, 3), 'elephant': (1, 8), 'rat': (2, 3)}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's return the count and the length of words:\n",
    "wordPairs = wordsRDD.map(lambda i: (i,i))\n",
    "wordPairs.combineByKey(\n",
    "    # combiner: for each word, we create a tuple of (1,len(word))\n",
    "    lambda w: (1,len(w)),                   \n",
    "    # merge value: \n",
    "    lambda tup,w: (tup[0]+1,tup[1]),       \n",
    "    # merge combiners: adding to combiners\n",
    "    lambda tup1,tup2: (tup1[0]+tup2[0],tup1[1])\n",
    ").collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': (2, ['cat', 'cat']),\n",
       " 'elephant': (1, ['elephant']),\n",
       " 'rat': (2, ['rat', 'rat'])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's return the count and the list of words:\n",
    "wordPairs = wordsRDD.map(lambda i:(i,i))\n",
    "wordPairs.combineByKey(\n",
    "    lambda w: (1,[w]),\n",
    "    lambda tup,w: (tup[0]+1,tup[1]+[w]),\n",
    "    lambda tup1,tup2: (tup1[0]+tup2[0],tup1[1]+tup2[1])\n",
    ").collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (2d) Apply word count to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2d.1) `wordCount` function **\n",
    "First, define a function for word counting. This function should take in an RDD that is a list of words like `wordsRDD` and return a pair RDD that has all of the words and their associated counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def wordCount(wordListRDD):\n",
    "    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n",
    "\n",
    "    Args:\n",
    "        wordListRDD (RDD of str): An RDD consisting of words.\n",
    "\n",
    "    Returns:\n",
    "        RDD of (str, int): An RDD consisting of (word, count) tuples.\n",
    "    \"\"\"\n",
    "    return wordListRDD.map(lambda w: (w,1)).reduceByKey(add)\n",
    "    \n",
    "print wordCount(wordsRDD).collect()\n",
    "assert sorted(wordCount(wordsRDD).collect()) == [('cat', 2), ('elephant', 1), ('rat', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2d.2) Capitalization and punctuation **\n",
    "Real world files are more complicated than the data we have been using in this lab. Some of the issues we have to address are:\n",
    "  + Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word).\n",
    "  + All punctuation should be removed.\n",
    "  + Any leading or trailing spaces on a line should be removed.\n",
    " \n",
    "Define the function `removePunctuation` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces.  Use the Python [re](https://docs.python.org/2/library/re.html) module to remove any text that is not a letter, number, or space. Reading `help(re.sub)` might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi you\n",
      "no underscore\n",
      "the elephants 4 cats\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "import re\n",
    "def removePunctuation(text):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    return re.sub('[^a-zA-Z0-9 ]','',text.lower().strip())\n",
    "\n",
    "assert removePunctuation(\" The Elephant's 4 cats. \") == 'the elephants 4 cats'\n",
    "print removePunctuation('Hi, you!')\n",
    "print removePunctuation(' No under_score!')\n",
    "print removePunctuation(\" The Elephant's 4 cats. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2d.3) Load a text file **\n",
    "For the next part of this lab, we will use the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We also apply the recently defined `removePunctuation()` function using a `map()` transformation to strip out the punctuation and change all text to lowercase.  Since the file is large we use `take(15)`, so that we only print 15 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1609\n",
      "1: \n",
      "2: the sonnets\n",
      "3: \n",
      "4: by william shakespeare\n",
      "5: \n",
      "6: \n",
      "7: \n",
      "8: 1\n",
      "9: from fairest creatures we desire increase\n",
      "10: that thereby beautys rose might never die\n",
      "11: but as the riper should by time decease\n",
      "12: his tender heir might bear his memory\n",
      "13: but thou contracted to thine own bright eyes\n",
      "14: feedst thy lights flame with selfsubstantial fuel\n"
     ]
    }
   ],
   "source": [
    "# Just run this code\n",
    "import os.path\n",
    "baseDir = os.path.join('../data') # wherever you have put the file 'shakespeare.txt'\n",
    "fileName = os.path.join(baseDir, 'shakespeare.txt')\n",
    "\n",
    "shakespeareRDD = (sc\n",
    "                  .textFile(fileName, 8)\n",
    "                  .map(removePunctuation))\n",
    "print '\\n'.join(shakespeareRDD\n",
    "                .zipWithIndex()  # to (line, lineNum)\n",
    "                .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'\n",
    "                .take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2d.4) Words from lines **\n",
    "Before we can use the `wordcount()` function, we have to address two issues with the format of the RDD:\n",
    "  + The first issue is that  that we need to split each line by its spaces.\n",
    "  + The second issue is we need to filter out empty lines.\n",
    " \n",
    "Apply a transformation that will split each element of the RDD by its spaces. For each element of the RDD, you should apply Python's string [split()](https://docs.python.org/2/library/string.html#string.split) function. You might think that a `map()` transformation is the way to do this, but think about what the result of the `split()` function will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']\n",
      "928908\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "shakespeareWordsRDD = shakespeareRDD.flatMap(lambda line: line.split(' '))\n",
    "shakespeareWordCount = shakespeareWordsRDD.count()\n",
    "print shakespeareWordsRDD.top(5)\n",
    "print shakespeareWordCount\n",
    "assert (shakespeareWordCount == 927631 or shakespeareWordCount == 928908)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2d.5) Remove empty elements **\n",
    "The next step is to filter out the empty elements.  Remove all entries where the word is `''`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1609', u'the', u'sonnets', u'by']\n",
      "882996\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "shakeWordsRDD = shakespeareWordsRDD.filter(lambda w: w !='')\n",
    "shakeWordCount = shakeWordsRDD.count()\n",
    "print shakeWordsRDD.take(4)\n",
    "print shakeWordCount\n",
    "assert shakeWordCount == 882996"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2d.6) Count the words **\n",
    "We now have an RDD that is only words.  Next, let's apply the `wordCount()` function to produce a list of word counts. We can view the top 15 words by using the `takeOrdered()` action; however, since the elements of the RDD are pairs, we need a custom sort function that sorts using the value part of the pair.\n",
    "\n",
    "You'll notice that many of the words are common English words (know as stopwords).\n",
    "\n",
    "Use the `wordCount()` function and `takeOrdered()` to obtain the fifteen most common words and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 27361\n",
      "and: 26028\n",
      "i: 20681\n",
      "to: 19150\n",
      "of: 17463\n",
      "a: 14593\n",
      "you: 13615\n",
      "my: 12481\n",
      "in: 10956\n",
      "that: 10890\n",
      "is: 9134\n",
      "not: 8497\n",
      "with: 7771\n",
      "me: 7769\n",
      "it: 7678\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "top15WordsAndCounts = wordCount(shakeWordsRDD).takeOrdered(15,key=lambda x: -x[1])\n",
    "print '\\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
