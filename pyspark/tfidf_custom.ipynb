{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying in a corpus of documents (of the web, for that matter?)\n",
    "\n",
    "### The purpose of this class is to understand how a corpus of documents can be indexed in a 'simple' way. After that, we can retrieve the most relevant documents for a given query, or we could also try to perform some clustering.\n",
    "\n",
    "### To this end, we are going to use a set of sonnets by Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\r\n",
      "\r\n",
      "THE SONNETS\r\n",
      "\r\n",
      "by William Shakespeare\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                     1\r\n",
      "  From fairest creatures we desire increase,\r\n",
      "  That thereby beauty's rose might never die,\r\n",
      "  But as the riper should by time decease,\r\n",
      "  His tender heir might bear his memory:\r\n",
      "  But thou contracted to thine own bright eyes,\r\n",
      "  Feed'st thy light's flame with self-substantial fuel,\r\n",
      "  Making a famine where abundance lies,\r\n",
      "  Thy self thy foe, to thy sweet self too cruel:\r\n",
      "  Thou that art now the world's fresh ornament,\r\n",
      "  And only herald to the gaudy spring,\r\n",
      "  Within thine own bud buriest thy content,\r\n",
      "  And tender churl mak'st waste in niggarding:\r\n",
      "    Pity the world, or else this glutton be,\r\n",
      "    To eat the world's due, by the grave and thee.\r\n",
      "\r\n",
      "\r\n",
      "                     2\r\n",
      "  When forty winters shall besiege thy brow,\r\n",
      "  And dig deep trenches in thy beauty's field,\r\n",
      "  Thy youth's proud livery so gazed on now,\r\n",
      "  Will be a tattered weed of small worth held:\r\n",
      "  Then being asked, where all thy beauty lies,\r\n",
      "  Where all the treasure of thy lusty days;\r\n",
      "  To say within thine own deep sunken eyes,\r\n",
      "  Were an all-eating shame, and thriftless praise.\r\n",
      "  How much more praise deserved thy beauty's use,\r\n",
      "  If thou couldst answer 'This fair child of mine\r\n",
      "  Shall sum my count, and make my old excuse'\r\n",
      "  Proving his beauty by succession thine.\r\n",
      "    This were to be new made when thou art old,\r\n",
      "    And see thy blood warm when thou feel'st it cold.\r\n",
      "\r\n",
      "\r\n",
      "                     3\r\n",
      "  Look in thy glass and tell the face thou viewest,\r\n",
      "  Now is the time that face should form another,\r\n",
      "  Whose fresh repair if now thou not renewest,\r\n",
      "  Thou dost beguile the world, unbless some mother.\r\n",
      "  For where is she so fair whose uneared womb\r\n",
      "  Disdains the tillage of thy husbandry?\r\n",
      "  Or who is he so fond will be the tomb,\r\n",
      "  Of his self-love to stop posterity?\r\n",
      "  Thou art thy mother's glass and she in thee\r\n",
      "  Calls back the lovely April of her prime,\r\n",
      "  So thou through windows of thine age shalt see,\r\n",
      "  Despite of wrinkles this thy golden time.\r\n",
      "    But if thou live remembered not to be,\r\n",
      "    Die single and thine image dies with thee.\r\n",
      "\r\n",
      "\r\n",
      "                     4\r\n"
     ]
    }
   ],
   "source": [
    "# path to the file. You should point this to your data\n",
    "file = '../data/shakespeare.txt'\n",
    "\n",
    "# see the document, using the 'head' command\n",
    "!head -n60 $file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that if we simply invoke the method `sc.textFile` on the input file, we will read the data line by line. For this excercise we want to create a corpus of documents; hence, we will read the data paragraph by paragraph, so that each paragraph will be like a document.\n",
    "\n",
    "### The method `newAPIHadoopFile` is quite advanced, so don't worry if you are not familiar with it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1609',\n",
       " u'THE SONNETS',\n",
       " u'by William Shakespeare',\n",
       " u'',\n",
       " u\"                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data by paragraphs\n",
    "paragraphs = sc.newAPIHadoopFile(file, \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\n",
    "    \"org.apache.hadoop.io.LongWritable\", \"org.apache.hadoop.io.Text\",\n",
    "    conf={\"textinputformat.record.delimiter\": '\\n\\n'}).map(lambda l:l[1])\n",
    "\n",
    "paragraphs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One we have our corpus of documents, we have to perform some cleaning on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the data: we want to remove spaces (larger than one), empty lines and punctuation\n",
    "import re\n",
    "\n",
    "cleanParagraphs = (paragraphs\n",
    "# remove punctuation \n",
    ".map(lambda p: re.sub('[^a-zA-Z0-9 ]','',p.lower().strip())) \n",
    "# replace multiple spaces by single space\n",
    ".map(lambda p: re.sub('[ ]+',' ',p))\n",
    "# remove empty lines from the RDD\n",
    ".filter(lambda p: p!='') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1609',\n",
       " u'the sonnets',\n",
       " u'by william shakespeare',\n",
       " u'1 from fairest creatures we desire increase that thereby beautys rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feedst thy lights flame with selfsubstantial fuel making a famine where abundance lies thy self thy foe to thy sweet self too cruel thou that art now the worlds fresh ornament and only herald to the gaudy spring within thine own bud buriest thy content and tender churl makst waste in niggarding pity the world or else this glutton be to eat the worlds due by the grave and thee',\n",
       " u'2 when forty winters shall besiege thy brow and dig deep trenches in thy beautys field thy youths proud livery so gazed on now will be a tattered weed of small worth held then being asked where all thy beauty lies where all the treasure of thy lusty days to say within thine own deep sunken eyes were an alleating shame and thriftless praise how much more praise deserved thy beautys use if thou couldst answer this fair child of mine shall sum my count and make my old excuse proving his beauty by succession thine this were to be new made when thou art old and see thy blood warm when thou feelst it cold',\n",
       " u'3 look in thy glass and tell the face thou viewest now is the time that face should form another whose fresh repair if now thou not renewest thou dost beguile the world unbless some mother for where is she so fair whose uneared womb disdains the tillage of thy husbandry or who is he so fond will be the tomb of his selflove to stop posterity thou art thy mothers glass and she in thee calls back the lovely april of her prime so thou through windows of thine age shalt see despite of wrinkles this thy golden time but if thou live remembered not to be die single and thine image dies with thee',\n",
       " u'4 unthrifty loveliness why dost thou spend upon thy self thy beautys legacy natures bequest gives nothing but doth lend and being frank she lends to those are free then beauteous niggard why dost thou abuse the bounteous largess given thee to give profitless usurer why dost thou use so great a sum of sums yet canst not live for having traffic with thy self alone thou of thy self thy sweet self dost deceive then how when nature calls thee to be gone what acceptable audit canst thou leave thy unused beauty must be tombed with thee which used lives th executor to be',\n",
       " u'5 those hours that with gentle work did frame the lovely gaze where every eye doth dwell will play the tyrants to the very same and that unfair which fairly doth excel for neverresting time leads summer on to hideous winter and confounds him there sap checked with frost and lusty leaves quite gone beauty oersnowed and bareness every where then were not summers distillation left a liquid prisoner pent in walls of glass beautys effect with beauty were bereft nor it nor no remembrance what it was but flowers distilled though they with winter meet leese but their show their substance still lives sweet',\n",
       " u'6 then let not winters ragged hand deface in thee thy summer ere thou be distilled make sweet some vial treasure thou some place with beautys treasure ere it be selfkilled that use is not forbidden usury which happies those that pay the willing loan thats for thy self to breed another thee or ten times happier be it ten for one ten times thy self were happier than thou art if ten of thine ten times refigured thee then what could death do if thou shouldst depart leaving thee living in posterity be not selfwilled for thou art much too fair to be deaths conquest and make worms thine heir',\n",
       " u'7 lo in the orient when the gracious light lifts up his burning head each under eye doth homage to his newappearing sight serving with looks his sacred majesty and having climbed the steepup heavenly hill resembling strong youth in his middle age yet mortal looks adore his beauty still attending on his golden pilgrimage but when from highmost pitch with weary car like feeble age he reeleth from the day the eyes fore duteous now converted are from his low tract and look another way so thou thy self outgoing in thy noon unlooked on diest unless thou get a son']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize some of the 'documents' to double check that everything went well\n",
    "cleanParagraphs.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the purpose of querying in the corpus, it's convinient to index the documents with `zipWithIndex`. See the differece with `zipWithUniqueId`.\n",
    "\n",
    "### Also, we are going to persist the data in main memory, since we may want to perform several queries. The  level of persistance can be changed usgin the `StorageLevel`, for instance `persist(StorageLevel.MEMORY_AND_DISK)`. In order to see the level of persistance you can use the method `getStorageLevel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[12] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add index to 'documents'. \n",
    "cleanParagraphsWithIndex = (cleanParagraphs\n",
    "                            .zipWithIndex() \n",
    "                            .map(lambda (doc,i): (i,doc)))\n",
    "# persist the data in RAM \n",
    "cleanParagraphsWithIndex.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, u'1609'), (1, u'the sonnets'), (2, u'by william shakespeare')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanParagraphsWithIndex.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6376 \"documents\"\n"
     ]
    }
   ],
   "source": [
    "# number of documents (that is, paragraphs) in our corpus:\n",
    "print 'There are %d \"documents\"' % cleanParagraphsWithIndex.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our next task is to *label* each document. One simple approach is to use the well-known  *term frequency–inverse document frequency*, or TF-IDF. Recall what we have seen in the class, and have a look at wikipedia!!!\n",
    "\n",
    "### As we have seen during the lesson, PySpark implementation of tf-idf has some problems. Hence, I leave here a custom implementation. This works well for the data we are using, but it's not scalable!!! (it will fail on a larger corpus of documents). At the end of the excerssise I will suggest some improvements ;)\n",
    "\n",
    "### In the following, we will be calculating separatedly the TF and IDF terms in a scalable way. Then, we will combine them by *collecting* the IDF RDD (i.e., bringing the data to memory). As I've said, this solution works here, but might fail in a larger corpus..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TF: term frequency in each document\n",
    "\n",
    "### Basically, we want a histogram of words for each document in the corpus. In order to leverage the importance of larger documents, we use log-scale instead of the raw frequency of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate document frequency:\n",
    "from math import log\n",
    "def create_hist_for_document(doc):\n",
    "    '''\n",
    "    This method accepts a string (doc), and returns its histogram of words \n",
    "    '''\n",
    "    # split input string into array of words\n",
    "    listOfWords = doc.split(' ')\n",
    "    # return a dictionary of key = word and value = frequency of that word\n",
    "    dicc = dict((x, listOfWords.count(x)) for x in listOfWords)\n",
    "    # convert to logarithmic scale: this step is optional, we can use directly the frequencies\n",
    "    dicc = {k:(1+log(dicc[k]) if dicc[k]!=0 else 0.0) for k in dicc}\n",
    "    return dicc\n",
    "\n",
    "# we use assert to check the solution\n",
    "assert create_hist_for_document('1 1 1 23 23 hola hola hola 1 a') == \\\n",
    "    {'1': 1+log(4), 'a': 1, '23': 1+log(2), 'hola': 1+log(3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the function create_hist_for_document to each 'document' in the RDD\n",
    "tf = cleanParagraphsWithIndex.map(lambda (i,doc): (i,create_hist_for_document(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, {u'1609': 1.0}),\n",
       " (1, {u'sonnets': 1.0, u'the': 1.0}),\n",
       " (2, {u'by': 1.0, u'shakespeare': 1.0, u'william': 1.0}),\n",
       " (3,\n",
       "  {u'1': 1.0,\n",
       "   u'a': 1.0,\n",
       "   u'abundance': 1.0,\n",
       "   u'and': 2.09861228866811,\n",
       "   u'art': 1.0,\n",
       "   u'as': 1.0,\n",
       "   u'be': 1.0,\n",
       "   u'bear': 1.0,\n",
       "   u'beautys': 1.0,\n",
       "   u'bright': 1.0,\n",
       "   u'bud': 1.0,\n",
       "   u'buriest': 1.0,\n",
       "   u'but': 1.6931471805599454,\n",
       "   u'by': 1.6931471805599454,\n",
       "   u'churl': 1.0,\n",
       "   u'content': 1.0,\n",
       "   u'contracted': 1.0,\n",
       "   u'creatures': 1.0,\n",
       "   u'cruel': 1.0,\n",
       "   u'decease': 1.0,\n",
       "   u'desire': 1.0,\n",
       "   u'die': 1.0,\n",
       "   u'due': 1.0,\n",
       "   u'eat': 1.0,\n",
       "   u'else': 1.0,\n",
       "   u'eyes': 1.0,\n",
       "   u'fairest': 1.0,\n",
       "   u'famine': 1.0,\n",
       "   u'feedst': 1.0,\n",
       "   u'flame': 1.0,\n",
       "   u'foe': 1.0,\n",
       "   u'fresh': 1.0,\n",
       "   u'from': 1.0,\n",
       "   u'fuel': 1.0,\n",
       "   u'gaudy': 1.0,\n",
       "   u'glutton': 1.0,\n",
       "   u'grave': 1.0,\n",
       "   u'heir': 1.0,\n",
       "   u'herald': 1.0,\n",
       "   u'his': 1.6931471805599454,\n",
       "   u'in': 1.0,\n",
       "   u'increase': 1.0,\n",
       "   u'lies': 1.0,\n",
       "   u'lights': 1.0,\n",
       "   u'making': 1.0,\n",
       "   u'makst': 1.0,\n",
       "   u'memory': 1.0,\n",
       "   u'might': 1.6931471805599454,\n",
       "   u'never': 1.0,\n",
       "   u'niggarding': 1.0,\n",
       "   u'now': 1.0,\n",
       "   u'only': 1.0,\n",
       "   u'or': 1.0,\n",
       "   u'ornament': 1.0,\n",
       "   u'own': 1.6931471805599454,\n",
       "   u'pity': 1.0,\n",
       "   u'riper': 1.0,\n",
       "   u'rose': 1.0,\n",
       "   u'self': 1.6931471805599454,\n",
       "   u'selfsubstantial': 1.0,\n",
       "   u'should': 1.0,\n",
       "   u'spring': 1.0,\n",
       "   u'sweet': 1.0,\n",
       "   u'tender': 1.6931471805599454,\n",
       "   u'that': 1.6931471805599454,\n",
       "   u'the': 2.791759469228055,\n",
       "   u'thee': 1.0,\n",
       "   u'thereby': 1.0,\n",
       "   u'thine': 1.6931471805599454,\n",
       "   u'this': 1.0,\n",
       "   u'thou': 1.6931471805599454,\n",
       "   u'thy': 2.6094379124341005,\n",
       "   u'time': 1.0,\n",
       "   u'to': 2.386294361119891,\n",
       "   u'too': 1.0,\n",
       "   u'waste': 1.0,\n",
       "   u'we': 1.0,\n",
       "   u'where': 1.0,\n",
       "   u'with': 1.0,\n",
       "   u'within': 1.0,\n",
       "   u'world': 1.0,\n",
       "   u'worlds': 1.6931471805599454})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check everything went well\n",
    "tf.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inverse document frequency (IDF):\n",
    "\n",
    "### After having obtained the frequency of words in each document (the so-called TF), we may want to remove terms that are too common. For instance, words like *a*, *and* or *the* are very unlikely to represent a document. Recall from the classroom that such words are known as [*stop-words*](https://en.wikipedia.org/wiki/Stop_words).\n",
    "\n",
    "### The IDF index of a word is inversely proportional to the occurrance of that word in the corpus and thus, is useful for removing terms that appear frequently in our corpus. Note that we only take into account whether the word appear in a document, not how many times!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from math import log\n",
    "\n",
    "# get unique words in a document\n",
    "def word_in_doc(doc):\n",
    "    '''\n",
    "    This method return the unique words in a document. Since we want to \n",
    "    calculate the occurrance of words among documents, we return a list of \n",
    "    tuples (word,1). This structure will be useful later on in the reduce phase.\n",
    "    '''\n",
    "    list_words = doc.split(' ')\n",
    "    unique_words = set(list_words)\n",
    "    # emit a 1 for each word:\n",
    "    words_with_ones = map(lambda w: (w,1),unique_words)\n",
    "    return words_with_ones\n",
    "\n",
    "# note that we only count 'tal' once (because the sentence passed to words_in_doc represents a single 'document')\n",
    "assert word_in_doc('hola que tal 123 tal') == [('tal', 1), ('123', 1), ('que', 1), ('hola', 1)]\n",
    "\n",
    "# calculate inverse document frequency\n",
    "def inverse_doc_freq(rdd):\n",
    "    '''\n",
    "    This method calculates the inverse document frequency for each term in each document\n",
    "    Input: and rdd consisting of tuples of (doc_index, doc)\n",
    "    \n",
    "    '''\n",
    "    nunmberOfDocs = rdd.count()\n",
    "    \n",
    "    # get an RDD of words with the number of documents this word appears in\n",
    "    freqWordsInDocs = (rdd.flatMap(lambda (i,doc): word_in_doc(doc))\n",
    "                       .reduceByKey(add))\n",
    "    # inverse doc frequency: an RDD of tuples of the form (key=word,value=idf)\n",
    "    idf = freqWordsInDocs.map(lambda (w,f): (w,log(1+nunmberOfDocs/float(1+f))))\n",
    "    return idf\n",
    "\n",
    "# Check that everything went well (you can try other sentences, but be sure to change the assert statement accordingly!)\n",
    "test_corpus = sc.parallelize([(0,'hola que tal 123 tal'),(1,'hola mi amor amor'),(2,'hola que haces amor')])\n",
    "assert inverse_doc_freq(test_corpus).collectAsMap()  == {'haces': log(1+3.0/(1+1)), \n",
    "           'mi': log(1+3.0/(1+1)), '123': log(1+3.0/(1+1)), 'que': log(1+3.0/(1+2)), \n",
    "           'tal': log(1+3.0/(1+1)), 'amor': log(1+3.0/(1+2)), 'hola': log(1+3.0/(1+3))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF\n",
    "\n",
    "### Now, we have to combine the TF and IDF RDDs. Remember that TF is an RDD of tuples:\n",
    "###    (index_of_doc, dictionary of word frequencies)\n",
    "### and IDF is a RDD of tuples: \n",
    "###    (word, idf)\n",
    "\n",
    "### There are several approaches to combine both RDDs. For this excersise, we will follow the simplest approach: collect the IDF RDD as a dictionary. It should be clear that, for a sufficiently large corpus, bringing this data to memory can cause a memoryOutOfBounds exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this might be unsafe!!!\n",
    "idf = inverse_doc_freq(cleanParagraphsWithIndex).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some idf: [(u'fawn', 6.123432217756322), (u'', 6.969477337610134), (u'nunnery', 7.662154335573845), (u'redeemst', 8.067462667010057), (u'woods', 5.929745576710511), (u'spiders', 6.969477337610134), (u'hanging', 5.155164546504638), (u'offendeth', 8.067462667010057), (u'beadsmen', 8.067462667010057), (u'scold', 6.56448219113042)]\n"
     ]
    }
   ],
   "source": [
    "print 'Some idf: %s' % [(i,idf[i]) for i in idf][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[59] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put toghether the tf and the idf\n",
    "tfidf = tf.map(lambda (i,dicc): (i,{k:dicc[k]*idf[k] for k in dicc if k in idf}))\n",
    "tfidf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, {u'1609': 7.374629015218945}),\n",
       " (1, {u'sonnets': 6.969477337610134, u'the': 1.0498658171066235}),\n",
       " (2,\n",
       "  {u'by': 1.6648624464339654,\n",
       "   u'shakespeare': 5.102832630920091,\n",
       "   u'william': 4.454502322995931}),\n",
       " (3,\n",
       "  {u'1': 4.7797457706673905,\n",
       "   u'a': 1.2134235539353357,\n",
       "   u'abundance': 6.123432217756322,\n",
       "   u'and': 2.114356047254294,\n",
       "   u'art': 2.5707599490570026,\n",
       "   u'as': 1.596515267393861,\n",
       "   u'be': 1.4896053641677396,\n",
       "   u'bear': 2.806459785645822,\n",
       "   u'beautys': 5.468685043514473,\n",
       "   u'bright': 4.675317945890173,\n",
       "   u'bud': 6.1973836831408065,\n",
       "   u'buriest': 8.067462667010057,\n",
       "   u'but': 2.530633792033937,\n",
       "   u'by': 2.8188571571998016,\n",
       "   u'churl': 6.459278280280682,\n",
       "   u'content': 3.975228671495789,\n",
       "   u'contracted': 6.2772698595469025,\n",
       "   u'creatures': 5.053133931046207,\n",
       "   u'cruel': 4.596051816651253,\n",
       "   u'decease': 6.815483336200744,\n",
       "   u'desire': 3.6225870148083623,\n",
       "   u'die': 3.029119289586728,\n",
       "   u'due': 4.566056929747677,\n",
       "   u'eat': 4.122669748270193,\n",
       "   u'else': 2.9558027378167555,\n",
       "   u'eyes': 2.694685679117508,\n",
       "   u'fairest': 5.128652220526714,\n",
       "   u'famine': 6.2772698595469025,\n",
       "   u'feedst': 7.374629015218945,\n",
       "   u'flame': 5.585999438999818,\n",
       "   u'foe': 4.761552140902392,\n",
       "   u'fresh': 4.366355704053968,\n",
       "   u'from': 1.8151399178721186,\n",
       "   u'fuel': 6.969477337610134,\n",
       "   u'gaudy': 6.815483336200744,\n",
       "   u'glutton': 7.374629015218945,\n",
       "   u'grave': 3.7860306899767893,\n",
       "   u'heir': 4.415746545922316,\n",
       "   u'herald': 4.896595244718707,\n",
       "   u'his': 2.586918042833683,\n",
       "   u'in': 1.3032509405001136,\n",
       "   u'increase': 5.432473563941401,\n",
       "   u'lies': 3.4094203608425873,\n",
       "   u'lights': 5.506269182508326,\n",
       "   u'making': 4.415746545922316,\n",
       "   u'makst': 5.767695800399854,\n",
       "   u'memory': 4.761552140902392,\n",
       "   u'might': 4.8729030255388315,\n",
       "   u'never': 2.3888197709155032,\n",
       "   u'niggarding': 8.067462667010057,\n",
       "   u'now': 1.7463229921588999,\n",
       "   u'only': 3.296743484380119,\n",
       "   u'or': 1.8980092690707653,\n",
       "   u'ornament': 5.929745576710511,\n",
       "   u'own': 4.397789579701992,\n",
       "   u'pity': 3.674010419741786,\n",
       "   u'riper': 7.374629015218945,\n",
       "   u'rose': 4.917492975547734,\n",
       "   u'self': 7.0450029010086395,\n",
       "   u'selfsubstantial': 8.067462667010057,\n",
       "   u'should': 2.119711659806393,\n",
       "   u'spring': 4.743689120123653,\n",
       "   u'sweet': 2.6137976628560207,\n",
       "   u'tender': 6.8728411934861295,\n",
       "   u'that': 2.3576099338504912,\n",
       "   u'the': 2.9309728363262657,\n",
       "   u'thee': 1.9408190340930642,\n",
       "   u'thereby': 5.506269182508326,\n",
       "   u'thine': 5.1741663199080055,\n",
       "   u'this': 1.5261054025278082,\n",
       "   u'thou': 3.0596581892459267,\n",
       "   u'thy': 5.01490888128333,\n",
       "   u'time': 2.344873467474797,\n",
       "   u'to': 2.9950870514689427,\n",
       "   u'too': 2.253821666435075,\n",
       "   u'waste': 5.029192203386434,\n",
       "   u'we': 1.8419627998513899,\n",
       "   u'where': 2.2005774723465303,\n",
       "   u'with': 1.3118917465453501,\n",
       "   u'within': 2.9134785623926396,\n",
       "   u'world': 2.73601157062015,\n",
       "   u'worlds': 8.326039366971983})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that until we don't execute this cell, nothing is actually done\n",
    "tfidf.take(4)\n",
    "# also, note that if you execute the cell again, the result appears inmediatly, because we persisted the RDD tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Querying\n",
    "\n",
    "### We are finally in a position to perform our queries on the corpus. The idea is to retrieve the most relevant docuemnts for a given set of words (i.e, the query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_paragraphs(tfidf =None, query = None, numberOfDocs = 10):\n",
    "    return (tfidf\n",
    "           .map(lambda (i,dicc): (i,sum({dicc[k] if k in dicc else -1 for k in query}) ))\n",
    "           .takeOrdered(numberOfDocs,key=lambda x: -x[1]))\n",
    "\n",
    "def return_docs(query = None, corpus = None, tfidf = None, numberOfDocs =10):\n",
    "    p = get_paragraphs(tfidf, query, numberOfDocs)\n",
    "    print 'Top-%s paragraphs are %s' % (numberOfDocs,p)\n",
    "    \n",
    "    indexes = [i[0] for i in p]\n",
    "    return corpus.filter(lambda (i,doc): i in indexes).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 paragraphs are [(3998, 17.22355811195959), (706, 15.517518234595325), (2366, 10.623747242651353), (1812, 9.629753774960644), (2, 9.557334953916023)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, u'by william shakespeare'),\n",
       " (706,\n",
       "  u'touchstone it is meat and drink to me to see a clown by my troth we that have good wits have much to answer for we shall be flouting we cannot hold william good evn audrey audrey god ye good evn william william and good evn to you sir touchstone good evn gentle friend cover thy head cover thy head nay prithee be coverd how old are you friend william five and twenty sir touchstone a ripe age is thy name william william william sir touchstone a fair name wast born i th forest here william ay sir i thank god touchstone thank god a good answer art rich william faith sir so so touchstone so so is good very good very excellent good and yet it is not it is but so so art thou wise william ay sir i have a pretty wit touchstone why thou sayst well i do now remember a saying the fool doth think he is wise but the wise man knows himself to be a fool the heathen philosopher when he had a desire to eat a grape would open his lips when he put it into his mouth meaning thereby that grapes were made to eat and lips to open you do love this maid william i do sir touchstone give me your hand art thou learned william no sir touchstone then learn this of me to have is to have for it is a figure in rhetoric that drink being pourd out of cup into a glass by filling the one doth empty the other for all your writers do consent that ipse is he now you are not ipse for i am he william which he sir touchstone he sir that must marry this woman therefore you clown abandon which is in the vulgar leave the society which in the boorish is company of this female which in the common is woman which together is abandon the society of this female or clown thou perishest or to thy better understanding diest or to wit i kill thee make thee away translate thy life into death thy liberty into bondage i will deal in poison with thee or in bastinado or in steel i will bandy with thee in faction will oerrun thee with policy i will kill thee a hundred and fifty ways therefore tremble and depart audrey do good william william god rest you merry sir exit'),\n",
       " (1812,\n",
       "  u'davy here sir shallow davy davy davy davy let me see davy let me see davy let me see yea marry william cook bid him come hither sir john you shall not be excusd davy marry sir thus those precepts cannot be served and again sir shall we sow the headland with wheat shallow with red wheat davy but for william cook are there no young pigeons davy yes sir here is now the smiths note for shoeing and ploughirons shallow let it be cast and paid sir john you shall not be excused davy now sir a new link to the bucket must needs be had and sir do you mean to stop any of williams wages about the sack he lost the other day at hinckley fair shallow a shall answer it some pigeons davy a couple of shortleggd hens a joint of mutton and any pretty little tiny kickshaws tell william cook davy doth the man of war stay all night sir shallow yea davy i will use him well a friend i th court is better than a penny in purse use his men well davy for they are arrant knaves and will backbite davy no worse than they are backbitten sir for they have marvellous foul linen shallow well conceited davy about thy business davy davy i beseech you sir to countenance william visor of woncot against clement perkes o th hill shallow there is many complaints davy against that visor that visor is an arrant knave on my knowledge davy i grant your worship that he is a knave sir but yet god forbid sir but a knave should have some countenance at his friends request an honest man sir is able to speak for himself when a knave is not i have servd your worship truly sir this eight years an i cannot once or twice in a quarter bear out a knave against an honest man i have but a very little credit with your worship the knave is mine honest friend sir therefore i beseech you let him be countenancd shallow go to i say he shall have no wrong look about davy exit davy where are you sir john come come come off with your boots give me your hand master bardolph bardolph i am glad to see your worship shallow i thank thee with all my heart kind master bardolph to the page and welcome my tall fellow come sir john falstaff ill follow you good master robert shallow exit shallow bardolph look to our horses exeunt bardolph and page if i were sawed into quantities i should make four dozen of such bearded hermits staves as master shallow it is a wonderful thing to see the semblable coherence of his mens spirits and his they by observing of him do bear themselves like foolish justices he by conversing with them is turned into a justicelike servingman their spirits are so married in conjunction with the participation of society that they flock together in consent like so many wild geese if i had a suit to master shallow i would humour his men with the imputation of being near their master if to his men i would curry with master shallow that no man could better command his servants it is certain that either wise bearing or ignorant carriage is caught as men take diseases one of another therefore let men take heed of their company i will devise matter enough out of this shallow to keep prince harry in continual laughter the wearing out of six fashions which is four terms or two actions and a shall laugh without intervallums o it is much that a lie with a slight oath and a jest with a sad brow will do with a fellow that never had the ache in his shoulders o you shall see him laugh till his face be like a wet cloak ill laid up shallow within sir john falstaff i come master shallow i come master shallow exit'),\n",
       " (2366,\n",
       "  u'stafford rebellious hinds the filth and scum of kent markd for the gallows lay your weapons down home to your cottages forsake this groom the king is merciful if you revolt william stafford but angry wrathful and inclind to blood if you go forward therefore yield or die cade as for these silkencoated slaves i pass not it is to you good people that i speak oer whom in time to come i hope to reign for i am rightful heir unto the crown stafford villain thy father was a plasterer and thou thyself a shearman art thou not cade and adam was a gardener william stafford and what of that cade marry this edmund mortimer earl of march married the duke of clarence daughter did he not stafford ay sir cade by her he had two children at one birth william stafford thats false cade ay theres the question but i say tis true the elder of them being put to nurse was by a beggarwoman stoln away and ignorant of his birth and parentage became a bricklayer when he came to age his son am i deny it if you can dick nay tis too true therefore he shall be king smith sir he made a chimney in my fathers house and the bricks are alive at this day to testify it therefore deny it not stafford and will you credit this base drudges words that speaks he knows not what all ay marry will we therefore get ye gone william stafford jack cade the duke of york hath taught you this cade aside he lies for i invented it myself go to sirrah tell the king from me that for his fathers sake henry the fifth in whose time boys went to spancounter for french crowns i am content he shall reign but ill be protector over him dick and furthermore well have the lord says head for selling the dukedom of maine cade and good reason for thereby is england maind and fain to go with a staff but that my puissance holds it up fellow kings i tell you that that lord say hath gelded the commonwealth and made it an eunuch and more than that he can speak french and therefore he is a traitor stafford o gross and miserable ignorance cade nay answer if you can the frenchmen are our enemies go to then i ask but this can he that speaks with the tongue of an enemy be a good counsellor or no all no no and therefore well have his head william stafford well seeing gentle words will not prevail assail them with the army of the king stafford herald away and throughout every town proclaim them traitors that are up with cade that those which fly before the battle ends may even in their wivesand childrens sight be hangd up for example at their doors and you that be the kings friends follow me exeunt the two staffords and soldiers cade and you that love the commons follow me now show yourselves men tis for liberty we will not leave one lord one gentleman spare none but such as go in clouted shoon for they are thrifty honest men and such as would but that they dare not take our parts dick they are all in order and march toward us cade but then are we in order when we are most out of order come march forward exeunt'),\n",
       " (3998,\n",
       "  u'how now sir hugh no school today evans no master slender is let the boys leave to play quickly blessing of his heart mrs page sir hugh my husband says my son profits nothing in the world at his book i pray you ask him some questions in his accidence evans come hither william hold up your head come mrs page come on sirrah hold up your head answer your master be not afraid evans william how many numbers is in nouns william two quickly truly i thought there had been one number more because they say ods nouns evans peace your tattlings what is fair william william pulcher quickly polecats there are fairer things than polecats sure evans you are a very simplicity oman i pray you peace what is lapis william william a stone evans and what is a stone william william a pebble evans no it is lapis i pray you remember in your prain william lapis evans that is a good william what is he william that does lend articles william articles are borrowed of the pronoun and be thus declined singulariter nominativo hic haec hoc evans nominativo hig hag hog pray you mark genitivo hujus well what is your accusative case william accusativo hinc evans i pray you have your remembrance child accusativo hung hang hog quickly hanghog is latin for bacon i warrant you evans leave your prabbles oman what is the focative case william william ovocativo o evans remember william focative is caret quickly and thats a good root evans oman forbear mrs page peace evans what is your genitive case plural william william genitive case evans ay william genitive horum harum horum quickly vengeance of jennys case fie on her never name her child if she be a whore evans for shame oman quickly you do ill to teach the child such words he teaches him to hick and to hack which theyll do fast enough of themselves and to call horum fie upon you evans oman art thou lunatics hast thou no understandings for thy cases and the numbers of the genders thou art as foolish christian creatures as i would desires mrs page prithee hold thy peace evans show me now william some declensions of your pronouns william forsooth i have forgot evans it is qui quae quod if you forget your quis your quaes and your quods you must be preeches go your ways and play go mrs page he is a better scholar than i thought he was evans he is a good sprag memory farewell mistress page mrs page adieu good sir hugh exit sir hugh get you home boy come we stay too long exeunt')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = ['william','shakespeare']\n",
    "return_docs(query=query, corpus=cleanParagraphsWithIndex,tfidf=tfidf,numberOfDocs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Check out that the word *william* appears several times in the last documents. Thus, we are actually retrieving relevant documents :) \n",
    "\n",
    "### However, the last document has more than 20 occurrancies of the word *william*. It seems reasonable that this document should have been returned in a higher position, right? Can you think of an explanation for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next steps\n",
    "\n",
    "### - As you might have noticed, we are not considering plurals here. Can you think of a way to account for this?\n",
    "### - As highligthed throughout this notebook, our implementation of TF-IDF is not scalable, since we are collecting in main memory the IDF dictionary. \n",
    "\n",
    "### One way to alliviate this problem could be to split the IDF RDD into chunks (that we now for sure that will fit in memory), and join each of these chunks with the Term Frequency RDD in an iterative process. This is known as a *map-join*, since the join is performed in the mapping side, and it's use it typical when performing a join between a large RDD and a smaller one. \n",
    "\n",
    "### The implementation in Spark can be done with [broadcast variables](http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables). This is a more advanced feature of Spark, so this excercise could be a perfect topic for a Master's Thesis :) As an example, see the following  [explanation of a map-join in Spark](http://dmtolpeko.com/2015/02/20/map-side-join-in-spark/).\n",
    "\n",
    "### - As we saw during the lesson, Spark's implementation of TF-IDF is not very robust (actually, it's still marked as *experimental*). The reason for this is that, in order to reduce the complexity of the problem, they use the so called [*hashing trick*](https://en.wikipedia.org/wiki/Feature_hashing). However, as explained in [Spark's documentation](http://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf), such implementation \"suffers from potential hash collisions, where different raw features (i.e. words) may become the same term after hashing\". This is precisely what we observed in the class.\n",
    "\n",
    "### - Apart form querying on the corpus, one thing that could be done is to perform some clustering. For this, we could use the tf-idf index to calculate the distance between any pair of documents. Once this is done, we could build the [similarity matrix](https://en.wikipedia.org/wiki/Similarity_measure) and find clusters using, for instance, PCA, k-means, etc. \n",
    "\n",
    "### Another approach, more advanced and interesting, is to apply [topic model](https://en.wikipedia.org/wiki/Topic_model) to the corpus. The idea is to find the *latent* topics in the corpus. Every document might then belong to one or more topics (i.e., clusters). One famous algorithm for Topic Model is [*Latent Dirichlet Allocation*](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), and there are implementations of it in Python or R, as well as a [scalable version in Spark](http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda) (experimental, so be careful!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
